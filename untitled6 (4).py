# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ootmjCqx6BAHWDhR4pAiEtCf3duS20jM
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
!pip install squarify
!pip install plotly
import squarify
import plotly.express as px
from matplotlib_venn import venn2
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

data=pd.read_csv("/content/StudentPerformanceFactors.csv")

data_df=pd.DataFrame(data)

data_df.head()

data_df.shape

data_df.info()

#show how many row is null in each column
missing_values = print(data_df.isnull().sum())

#after we know info then we find columns(Teacher_Quality,Parental_Education_Level,Distance_from_Home) have null values
data_df=data_df.dropna(subset=['Teacher_Quality','Parental_Education_Level','Distance_from_Home'])

data_df.info()

data_df.describe()

data_df.duplicated().any() #there is no duplicated data

# Select only numeric columns to check if it has negative values or not
numeric_df = data_df.select_dtypes(include=['number'])

# Check for any negative values in numeric columns
negative_values = print((numeric_df < 0).any().any())

data_df.shape

print(data_df.nunique())

plt.figure(figsize=(10, 4)) #seaborn library
sns.boxplot(x='Hours_Studied', data=data_df,color='cyan')
plt.title('Box Plot of Hours Studied')
plt.xlabel('Hours Studied',fontsize=14)
plt.ylabel('Box Plot Values', fontsize=14)
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Attendance', data=data_df,color='cyan')
plt.title('Box Plot of Attendance')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Sleep_Hours', data=data_df,color='cyan')
plt.title('Box Plot of Sleep_Hours')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Previous_Scores', data=data_df,color='cyan')
plt.title('Box Plot of Previous_Scores')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Tutoring_Sessions', data=data_df,color='cyan')
plt.title('Box Plot of Tutoring_Sessions')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Physical_Activity', data=data_df,color='cyan')
plt.title('Box Plot of Physical_Activity')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x='Exam_Score', data=data_df,color='cyan')
plt.title('Box Plot of Exam_Score')
plt.show()

# Calculate IQR for numeric columns
Q1 = numeric_df.quantile(0.25) #25%
Q3 = numeric_df.quantile(0.75) #75%

#the normal data is the difference between Q1 and Q2=interquartile range
IQR = Q3 - Q1

# Identify outliers for numeric columns as outlier is a value higher or lower than 1.5*IQR
outliers = ((numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR)))

# Check if there are outliers and print the result
if outliers.any().any():
    print("Outliers detected in the following columns:")
    print(outliers.sum()) #shows each column have how many outlier values
else:
    print("No outliers detected in the dataset.")
# after running the code we found that there is a 40, 423 and 103 outliers row in Hours_Studied,Tutoring_Sessions and Tutoring_Sessions respectively

exam_score_outliers = data_df[(outliers['Exam_Score'] == True)]

# print outlier values in Exam_Score
print("Outliers in 'Exam_Score':")
print(exam_score_outliers[['Exam_Score']])
print('___________________________________________________________')

Hours_Studied_outliers = data_df[(outliers['Hours_Studied'] == True)]

# print outlier values in Hours_Studied
print("Outliers in 'Hours_Studied':")
print(exam_score_outliers[['Hours_Studied']])
print('___________________________________________________________')

Tutoring_Sessions_outliers = data_df[(outliers['Tutoring_Sessions'] == True)]

# print outlier values in Tutoring_Sessions
print("Outliers in 'Tutoring_Sessions':")
print(exam_score_outliers[['Tutoring_Sessions']])

cleaned_data_df = data_df[~outliers.any(axis=1)]
#axis=1 to check in rows not columns about its outlier value
#we use negation to reverse (if there is any outliers(true) then make it false(remove it))

print(f"Original dataset shape: {data_df.shape}")
print(f"Cleaned dataset shape (after removing outliers): {cleaned_data_df.shape}")

# the cleaned data is stored in the 'cleaned_data_df' variable

# 1. Check the first few rows of the dataset to make sure everything looks good
print(cleaned_data_df.head())

# 2. Select numerical columns
numerical_cols = cleaned_data_df.select_dtypes(include=[np.number]).columns

# 3. Standardize/scale the numerical data (mean=0 sd=1) to make data's feature close
scaler = StandardScaler()
scaled_data = scaler.fit_transform(cleaned_data_df[numerical_cols])

# 4. Convert the scaled data to a DataFrame for ease of analysis
scaled_df = pd.DataFrame(scaled_data, columns=numerical_cols)

# 5. Elbow Method to find the optimal number of cluster
inertia = [] #empty list and we add inertia values that represent sum of squared distances
for i in range(1, 11):  # Trying k from 1 to 10
    kmeans = KMeans(n_clusters=i, random_state=42) #42 can be changed with any constant(make me start from last center)
    kmeans.fit(scaled_df)
    inertia.append(kmeans.inertia_)

# Plot inertia to observe the elbow
plt.plot(range(1, 11), inertia)
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

# 6. Apply KMeans clustering (Assume k=3 for now, based on above analysis)
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(scaled_df)

# 7. Add the cluster labels to the original DataFrame
#each value classified to what group from 3 clusters
cleaned_data_df['Cluster'] = kmeans.labels_

# 8. Show the data with cluster labels
print(cleaned_data_df.head())

# 9. Visualize the clusters (only if you have 2 or 3 numerical features)
# Assuming there are at least two numerical features for visualization
if len(numerical_cols) >= 2:
    plt.scatter(cleaned_data_df[numerical_cols[0]], cleaned_data_df[numerical_cols[1]], c=cleaned_data_df['Cluster'], cmap='viridis')
    plt.xlabel(numerical_cols[0])
    plt.ylabel(numerical_cols[1])
    plt.title('Clusters Visualized')
    plt.show()

# 2.horizontal bar chart
# Group by 'School_Type' and calculate the mean Exam Score (or sum, max, etc. as needed)
sum_scores = cleaned_data_df.groupby('School_Type')['Exam_Score'].sum().reset_index()
#reset_index():turn the resulting grouped data back into a DataFrame.

# Create a horizontal bar chart
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed(w,l)
plt.barh(sum_scores['School_Type'], sum_scores['Exam_Score'], color=['#F0F8FF', '#E6E6FA', '#B0E0E6'])
#specify mean exam score for each school type
plt.title('Bar Chart School_Type vs. Exam_Score')
plt.xlabel('total Exam_Scores')
plt.ylabel('School_Type')
plt.show()

#3.column chart
mean_scores = cleaned_data_df.groupby('Distance_from_Home')['Attendance'].sum().reset_index()
#sum of attendance for each unique value of Distance_from_Home
# Create a column chart
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed
plt.bar(mean_scores['Distance_from_Home'], mean_scores['Attendance'], color=['#F0F8FF', '#E6E6FA', '#B0E0E6'])
plt.title('Chart Distance_from_Home vs. Attendance')
plt.xlabel('Distance_from_Home')
plt.ylabel('Attendance')
plt.show()

#4.pivot table
'''The code creates a summary table (pivot table) showing
the average exam scores gained by different genders of passengers ('gender')
in motivation level  ('Motivation_Level') in our data'''
cleaned_data_df_pivot_mean= pd.pivot_table(cleaned_data_df,values="Exam_Score",index="Gender",columns="Motivation_Level",
                                           aggfunc="mean")
cleaned_data_df_pivot_mean

# There are other aggregation functions like sum, median, var, std etc..
cleaned_data_df_pivot_sum= pd.pivot_table(cleaned_data_df,values="Exam_Score",index="Gender",columns="Motivation_Level",aggfunc="sum")
cleaned_data_df_pivot_sum

# You can also create your custom aggregation function:
def range_func(x):
 return x.max() - x.min()
cleaned_data_df_pivot_custom = pd.pivot_table(cleaned_data_df, values="Exam_Score", index="Gender",columns="Motivation_Level", aggfunc=range_func)
cleaned_data_df_pivot_custom

#grouped column chart
cleaned_data_df_pivot_sum.plot(kind="bar",alpha=0.5)
# alpha=0.5 sets the transparency of the bars.
plt.title('Chart gender vs. exam score')
plt.xlabel('gender')
plt.ylabel('Total scores')
plt.show()

# 5.groubed column chart
# Creating the bar chart
column1 = 'Motivation_Level'
column2 = 'Gender'

# Count occurrences for each combination of the two columns
counts = cleaned_data_df.groupby([column1, column2]).size().unstack()
# plt.barh(counts , color = ['#F0F8FF','#E6E6FA','#B0E0E6'])
counts.plot(kind='bar', figsize=(10, 6))

plt.title('Bar Chart Gender type vs. Motivation_Level')
plt.xlabel('Motivation_Level')
plt.show()

# 7.pie chart
# Creating sample data for a pie chart
gender_counts = cleaned_data_df['Gender'].value_counts()
plt.pie(gender_counts, labels=gender_counts.index, colors=['#ff9999','#66b3ff'])
plt.title('male and female Pie Chart')
plt.show()

# Prepare the data for a stacked column chart
stack_data = cleaned_data_df.groupby(['Gender', 'School_Type'])['Exam_Score'].mean().unstack()  #Computes the mean (average) of the Exam_Score column for each group defined by Gender and School_Type.
#.unstack(): Rearranges the grouped data so that School_Type categories form separate columns, making it easier to create a stacked bar chart.

# Plot the stacked column chart
stack_data.plot(kind='bar', stacked=True, figsize=(7, 6), colormap='viridis')

# Customize the chart
plt.title('Average Exam Score by Gender and School Type', fontsize=14)
plt.xlabel('Gender', fontsize=12)
plt.ylabel('Average Exam Score', fontsize=12)
plt.legend(title='School Type', fontsize=10)
plt.xticks(rotation=0)
plt.tight_layout()

# Show the chart
plt.show()

# Prepare the data for an area chart (tracking Exam_Score over Hours_Studied)
x = cleaned_data_df.Hours_Studied
y = cleaned_data_df.Exam_Score

data['Hours_Studied_bins'] = pd.cut(data['Hours_Studied'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
area_data = data.groupby('Hours_Studied_bins')['Exam_Score'].mean()
ax=plt.gca() # Get the current Axes instance
# Creating the areachart
area_data.plot(kind='area', figsize=(10, 6), color='skyblue', alpha=0.7)
plt.title('Average Exam Score by Hours Studied Groups', fontsize=14)
plt.xlabel('Hours Studied Groups', fontsize=12)
plt.ylabel('Average Exam Score', fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()

#Creating the scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(cleaned_data_df['Exam_Score'],cleaned_data_df['Attendance'], color='#FF69B4' )
plt.title('Scatter Plot of Exam_Score vs Attendance',fontsize=14)
plt.xlabel('Attendance',fontsize=12)
plt.ylabel('Exam_Score',fontsize=12)
plt.grid(alpha=0.4)   #background grid with light color
plt.tight_layout()
# Show the scatter plot
plt.show()

# Creating the dataset
#This ensures that the random numbers generated are the same every time the
x = cleaned_data_df.Hours_Studied
y = cleaned_data_df.Attendance
area = cleaned_data_df.Exam_Score
df = pd.DataFrame({'X': x, 'Y': y , "bubble_size":area})
df.head()
#Creating the bubble chart
scatter = plt.scatter('X', 'Y', s='bubble_size', alpha=0.5, data=df,c='lightblue')
plt.title('Hours Studied VS Attendance')
plt.xlabel('Hours Studied')
plt.ylabel('Attendance')
plt.show()
#large size of bubble determine exam score

# Creating the box plot
plt.figure(figsize=(10, 4))
sns.boxplot(x=cleaned_data_df['Motivation_Level'], y=cleaned_data_df['Attendance'], data=cleaned_data_df, palette='Set2')

# Adding titles and labels
plt.title('Box Plot of Attendance by Motivation Level')
plt.xlabel('Motivation Level')
plt.ylabel('Attendance')

# Show the plot
plt.show()

mean = 19.977109
stdv=5.985460
 #generates 1000 random data points from a normal distribution with the specified mean and standard deviatio
dist= pd.DataFrame(np.random.normal(loc=mean, scale=stdv, size=(1000, 1)),
columns=['rnd_data'])
 # size=(1000, 1) creates 1000 rows and 1 columns of normally distributed data.
print(dist.agg(['min', 'max', 'mean', 'std']).round(decimals=2))
dist.head()

# cleaned_data_df = {
#  'Parental_Involvement': ['Low', 'Low', 'Medium',
#  'Low', 'Medium'],
# 'Hours_Studied':[23,19,24,29,19],
#  'Access_to_Resources': ['High', 'Medium', 'Medium','Medium','Medium'],

# 'Motivation_Level': ['Low', 'Low', 'Medium',
#  'Medium', 'Medium']

#  ,'Exam_Score':[67,61,74,71,70] }
# cleaned_df = pd.DataFrame(cleaned_data_df)
# cleaned_df

# Summing sales by Category to get sizes for the highest hierarchy level
Hours_Studiedmotiv = cleaned_data_df.groupby('Motivation_Level')['Hours_Studied'].sum().reset_index()
Hours_Studiedmotiv

# Define a list of colors (one for each category)
colors = ['#ff9999', '#66b3ff', '#99ff99']
squarify.plot(sizes=Hours_Studiedmotiv['Hours_Studied'], label=Hours_Studiedmotiv['Motivation_Level'],
alpha=0.8, color=colors)
plt.axis('off')
plt.title('Hours_Studied Treemap by Motivation_Level')
plt.show()

# Assuming 'Hours_Studiedmotiv' is the DataFrame created earlier
 # Creating the tree map
fig = px.treemap(cleaned_data_df,
path=['Parental_Involvement', 'Exam_Score'], # Hierarchical levels: Parental_Involvement Region
values='Hours_Studied',
color='Hours_Studied',
# Size of the rectangles
# Color scale based on Hours_Studied
color_continuous_scale='RdBu',# Color scheme
title='Hours_Studied Treemap by Parental_Involvement and Exam_Score')
# Showing the figure
fig.show()

# Define your sets
Exam_Score = set([67,61,74,71,70])
Previous_Scores  = set([71,59,91,98,65])
# Create the Venn diagram
venn2([Exam_Score, Previous_Scores ], ('Exam_Score', 'Previous_Scores '))
# Display the diagram
plt.show()